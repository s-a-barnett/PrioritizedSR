{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b139c1b",
   "metadata": {},
   "source": [
    "# Learning SR-Based Representations\n",
    "\n",
    "In this notebook, we train an agent navigating in a grid-like environment instantiated with a simple pattern. The agent can act with a particular goal in mind (defined by a location with a reward of +1), it can act uniformly in all directions, or a mixture between the two. While the agent acts in the world, it updates both its SR, `M(a, s, s')`, and the state-based reward vector `w(s)`. It also learns an online approximation of the `K` largest eigenvectors.\n",
    "\n",
    "During the training, it is able to replay certain transitions according to its recorded experience, using either a Dyna or Prioritized Sweeping based approach.\n",
    "\n",
    "The algorithm employed for SR updates is a variant on SARSA, meaning that updates are made online, using the actual subsequent action taken. However, for memory-based updates, the subsequent action in the Bellman equation is chosen according to the agent's policy. For `epsilon = 0` or for large values of `beta`, this is the offline Q-learning style approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b73c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from gridworld import SimpleGrid\n",
    "from algs import TDSR\n",
    "import progressbar\n",
    "from plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a061835",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 7\n",
    "pattern=\"empty\" # \"empty\" or \"four_rooms\"\n",
    "env = SimpleGrid(grid_size, block_pattern=pattern, obs_mode=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env episode params\n",
    "episode_length = (2 * grid_size) ** 2\n",
    "episodes = 10\n",
    "goal_pos = [grid_size // 2, grid_size // 2]\n",
    "\n",
    "# agent params\n",
    "gamma = 0.9\n",
    "lr = 5e-2 # learning rate across all incremental algs\n",
    "poltype = 'egreedy' # \"egreedy\" or \"softmax\"\n",
    "epsilon = 1 # for epsilon-greedy policies\n",
    "beta = 20 # inverse temperature for softmax policies\n",
    "\n",
    "# memory params\n",
    "n_mem = 100 # number of retrieved memories per timestep\n",
    "theta = 5e-1 # priority threshold for prioritized sweeping\n",
    "memtype = \"ps\" # None or \"dyna\" or \"ps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory helper functions \n",
    "\n",
    "def memory_update(exp, agent, epsilon, beta):\n",
    "    exp1 = exp.copy()\n",
    "    if not exp[-1]:\n",
    "        # change to \"best\" action in hindsight\n",
    "        exp1[1] = agent.sample_action(exp[0], epsilon=epsilon, beta=beta)\n",
    "    td_sr = agent.update_sr(exp, exp1)\n",
    "    return td_sr\n",
    "\n",
    "def exp_normalize(x):\n",
    "    b = x.max()\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum()\n",
    "\n",
    "def get_dyna_indices(experiences, weights, nsamples):\n",
    "    p = exp_normalize(np.array(weights))\n",
    "    p /= p.sum()\n",
    "    return npr.choice(len(experiences), nsamples, p=p, replace=True)\n",
    "\n",
    "def get_predecessors(state, experiences):\n",
    "    return [exp for exp in experiences if (exp[2] == state)]\n",
    "\n",
    "def queue_append(exp, priority, queue):\n",
    "    \n",
    "    already_in_queue = False\n",
    "    # if exp is already in queue with a lower priority, replace with higher priority\n",
    "    for mem in queue:\n",
    "        if ((mem[\"exp\"] == exp) and (mem[\"priority\"] < priority)):\n",
    "            already_in_queue = True\n",
    "            mem[\"priority\"] = priority\n",
    "    \n",
    "    if not already_in_queue:\n",
    "        queue.append({\"exp\": exp, \"priority\": priority})\n",
    "        \n",
    "    return queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebee842",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TDSR(env.state_size, env.action_size, lr, gamma, poltype=poltype)\n",
    "experiences = []\n",
    "exp_weights = []\n",
    "pqueue = []\n",
    "errors = []\n",
    "\n",
    "for i in progressbar.progressbar(range(episodes)):\n",
    "    \n",
    "    # start agent in a random position with the same goal\n",
    "    env.reset(goal_pos=goal_pos)\n",
    "    state = env.observation\n",
    "\n",
    "    for j in range(episode_length):\n",
    "        action = agent.sample_action(state, epsilon=epsilon, beta=beta)\n",
    "        reward = env.step(action)\n",
    "        state_next = env.observation\n",
    "        done = env.done\n",
    "        experiences.append([state, action, state_next, reward, done])        \n",
    "        state = state_next\n",
    "        \n",
    "        if (j > 1):\n",
    "            td_sr = agent.update_sr(experiences[-2], experiences[-1])\n",
    "            errors.append(np.linalg.norm(td_sr))\n",
    "            if (memtype == 'ps'):\n",
    "                if (np.linalg.norm(td_sr) > theta):\n",
    "                    pqueue = queue_append(experiences[-2], np.linalg.norm(td_sr), pqueue)\n",
    "            td_w = agent.update_w(experiences[-1])\n",
    "            grad = agent.update_eigs(experiences[-2])\n",
    "                         \n",
    "        if env.done:\n",
    "            td_sr = agent.update_sr(experiences[-1], experiences[-1])\n",
    "            errors.append(np.linalg.norm(td_sr))\n",
    "            if (memtype == 'ps'):\n",
    "                if (np.linalg.norm(td_sr) > theta):\n",
    "                    pqueue = queue_append(experiences[-1], np.linalg.norm(td_sr), pqueue)\n",
    "            grad = agent.update_eigs(experiences[-1])\n",
    "            \n",
    "        if (memtype == 'dyna'):\n",
    "            exp_weights.append(len(experiences))\n",
    "            mem_indices = get_dyna_indices(experiences, exp_weights, n_mem)\n",
    "            mem = [experiences[t] for t in mem_indices]\n",
    "            for exp in mem:\n",
    "                td_sr = memory_update(exp, agent, epsilon, beta)\n",
    "                grad = agent.update_eigs(exp)\n",
    "            \n",
    "        elif (memtype == 'ps'):\n",
    "            for _ in range(n_mem):\n",
    "                \n",
    "                if not pqueue:\n",
    "                    break\n",
    "                    \n",
    "                # get highest priority item and remove\n",
    "                pqueue = sorted(pqueue, key = lambda item: item[\"priority\"])\n",
    "                exp = pqueue.pop()[\"exp\"]\n",
    "                \n",
    "                td_sr = memory_update(exp, agent, epsilon, beta)                \n",
    "                grad = agent.update_eigs(exp)\n",
    "                \n",
    "                for exp_pred in get_predecessors(exp[0], experiences):\n",
    "                    td_sr = memory_update(exp_pred, agent, epsilon, beta)                    \n",
    "                    grad = agent.update_eigs(exp_pred)\n",
    "                    if (np.linalg.norm(td_sr) > theta):\n",
    "                        pqueue = queue_append(exp_pred, np.linalg.norm(td_sr), pqueue)\n",
    "                        \n",
    "        if env.done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af3f9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for larger grid sizes, this plot might be too unwieldy\n",
    "if grid_size <= 10:\n",
    "    plot_place_fields(agent, env, epsilon=epsilon, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(agent.get_M_states(epsilon=epsilon, beta=beta).copy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff5fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_grid_fields(agent, env, online=False, epsilon=epsilon, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f7861",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_grid_fields(agent, env, online=True, epsilon=epsilon, beta=beta, nrows=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a5dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
